<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="A System for Imitation Learning of Contact-Rich Bimanual Manipulation Policies">
    <meta name="keywords" content="Bimanual, Manipulation, IROS 2022">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Bimanual Manipulation</title>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
    <nav class="navbar" role="navigation" aria-label="main navigation">
        <div class="navbar-brand">
            <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu">
            <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
                <a class="navbar-item" href="https://simonstepputtis.com">
                    <span class="icon">
                        <i class="fas fa-home"></i>
                    </span>
                </a>
                <div class="navbar-item has-dropdown is-hoverable">
                    <a class="navbar-link">
                        More Research
                    </a>
                    <div class="navbar-dropdown">
                        <a class="navbar-item" href="https://interactive-robotics.engineering.asu.edu/interaction-primitives/">
                            Interaction Primitives
                        </a>
                        <a class="navbar-item" href="https://simonstepputtis.com/neurips-2020-lcim/">
                            Language Conditioned Imitation Learning
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </nav>
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">A System for Imitation Learning of Contact-Rich Bimanual Manipulation Policies</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://simonstepputtis.com">Simon Stepputtis</a><sup>1</sup>,</span>
                            <span class="author-block">
                                Maryam Bandari<sup>2</sup>,</span>
                            <span class="author-block">
                                Stefan Schaal<sup>2</sup>,
                            </span>
                            <span class="author-block">
                                <a href="http://henibenamor.weebly.com/">Heni Ben Amor</a><sup>3</sup>
                            </span>
                        </div>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>Carnegie Melon University,</span>
                            <span class="author-block"><sup>2</sup>Intrinsic, An Alphabet Company,</span>
                            <span class="author-block"><sup>3</sup>Arizona State University</span>
                        </div>
                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/pdf/2208.00596.pdf"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                <span>arXiv Paper</span>
                                </a>
                                </span>
                                <!-- Teaser Video Link. -->
                                <span class="link-block">
                                    <a href="https://youtu.be/yElviO6vtV0"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-youtube"></i>
                                        </span>
                                <span>Teaser Video</span>
                                </a>
                                </span>
                                <!-- Presentation Video Link. -->
                                <span class="link-block">
                                    <a href="https://youtu.be/eLGUmv46I30"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-youtube"></i>
                                        </span>
                                <span>Presentation</span>
                                </a>
                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/ir-lab/irl_control/tree/iros2022-dev/irl_control/learning"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                <span>Library Code</span>
                                </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered">

                <!-- Visual Effects. -->
                <div class="column">
                    <div class="content">
                        <img src="static/assets/sim-task.gif" alt="Simulation task" width="100%">
                    </div>
                </div>
                <!--/ Visual Effects. -->

                <!-- Matting. -->
                <div class="column">
                    <div class="columns is-centered">
                        <div class="column content">
                            <img src="static/assets/sim-task-insert.gif" alt="Simulation close-up insertion" width="100%">
                        </div>
                    </div>
                </div>
            </div>

            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            In this paper, we discuss a framework for teaching bimanual manipulation tasks by imitation.
                        </p>
                        <p>
                            To this end, we present a system and algorithms for learning compliant and contact-rich robot behavior from human demonstrations. The presented system combines insights from admittance control and machine learning to extract control policies that can
                            (a) recover from and adapt to a variety of disturbances in time and space while also (b) effectively leveraging physical contact with the environment.
                        </p>
                        <p>
                            We demonstrate the effectiveness of our approach using a real-world insertion task involving multiple simultaneous contacts between a manipulated object and insertion pegs. We also investigate efficient means of collecting training data for such bimanual
                            settings. To this end, we conduct a human-subject study and analyze the effort and mental demand as reported by the users. Our experiments show that, while harder to provide, the additional force/torque information available
                            in teleoperated demonstrations is crucial for phase estimation and task success. Ultimately, force/torque data substantially improves manipulation robustness, resulting in a 90\% success rate in a multipoint insertion task.
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->

            <!-- Paper video. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Introduction Video (3 Minutes)</h2>
                    <div class="publication-video">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/yElviO6vtV0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    </div>
                </div>
            </div>
            <!--/ Paper video. -->
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop content">

            <div class="rows">

                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dcliport">Bimanual Manipulation</span></h2>

                        <h3 class="title is-4">Full IROS 2022 Presentation (7 Minutes)</h3>
                        <div class="content has-text-justified">
                            <!-- Paper Presentation. -->
                            <div class="columns is-centered has-text-centered">
                                <div class="column is-four-fifths">
                                    <div class="publication-video">
                                        <iframe width="560" height="315" src="https://www.youtube.com/embed/eLGUmv46I30" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                    </div>
                                </div>
                            </div>
                            <!--/ Paper Presentation. -->
                        </div>

                        <h3 class="title is-4">Approach</h3>
                        <div class="content has-text-justified">
                            <p>
                                In our approach, we combine multimodal sensor data acquired from force/torque sensors attached to each robot's wrist, visual object tracking, and robot joint states to learn a Bayesian Interaction Primitive. The learned model then infers the temporal
                                and spatial progress of the insertion task in terms of a phase value \(\phi\). This phase \(0 \leq \phi \leq 1\) describes the progress of the task, allowing us to represent varying trajectory lengths in the real world
                                on a unified scale. The phase estimation is key to enabling temporal inference over the task progress to appropriately react to perturbations.
                            </p>
                        </div>
                        <!-- <section class="section"> -->
                        <div class="container is-max-desktop">

                            <div class="columns is-centered">

                                <!-- Visual Effects. -->
                                <div class="column">
                                    <div class="content">
                                        <h2 class="title is-5">Phase Estimation During Perturbation</h2>
                                        <img src="static/assets/phase.gif" alt="Phase Estimation" width="100%">
                                        <p>
                                            When exposed to external perturbations or facing difficulties during the insertion, the phase estimate \(\phi\) is continuously adjusted.
                                        </p>
                                    </div>
                                </div>
                                <!--/ Visual Effects. -->

                                <!-- Matting. -->
                                <div class="column">
                                    <h2 class="title is-5">Motion Consistency</h2>
                                    <div class="columns is-centered">
                                        <div class="column content">
                                            <img src="static/assets/phase_joints.gif" alt="Motion Consistency" width="100%">
                                            <p>
                                                When the phase \(\phi\) is adjusted by the algorithm, predicted motions remain in a similar motion domain, even when pused outside of demonstrated behavior.
                                            </p>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <!--/ Matting. -->
                            <div class="content has-text-justified">
                                <p>
                                    When misalignments or other external perturbations are detected, the phase estimation, together with the underlying admittance controller, are used to overcome obstructions. This allows the system to correct and even reverse the estimated task progress,
                                    resulting in multiple insertion attempts. At its core, the combination of phase estimation and adaptive admittance control enables smooth insertions, as shown in the videos above.
                                </p>
                                <p>
                                    To train the bayesian interaction primitive, we collect thirty demonstrations in the real world by moving the bracket, either with a space mouse, allowing for six-dimensional manipulation, or by utilizing kinesthetic teaching.
                                </p>
                            </div>

                            <!-- <section class="section"> -->
                            <div class="container is-max-desktop">

                                <div class="columns is-centered">

                                    <!-- Visual Effects. -->
                                    <div class="column">
                                        <div class="content">
                                            <h2 class="title is-5">Teleoperation (Space-Mouse)</h2>
                                            <!-- <p>
                                                When teleoperating with a Space-Mouse, users need to initially familiarize themselves with the control; however, all force/torque sensor data can be attributed to the interaction with the environment and be used during inference.
                                            </p> -->
                                            <img src="static/assets/spacemouse.gif" alt="Teleoperation" width="100%">
                                        </div>
                                    </div>
                                    <!--/ Visual Effects. -->

                                    <!-- Matting. -->
                                    <div class="column">
                                        <h2 class="title is-5">Kinesthetic Teaching</h2>
                                        <div class="columns is-centered">
                                            <div class="column content">
                                                <!-- <p>
                                                    Using kinesthetic teaching, users report more intuitive maneuvering; however, force/torque sensor data are used to enable the kinesthetic teaching and can thus not be used during inference.
                                                </p> -->
                                                <img src="static/assets/kinesthetic.gif" alt="Kinesthetic Teaching" width="100%">
                                            </div>

                                        </div>
                                    </div>
                                </div>
                                <!--/ Matting. -->

                                <br>
                                <div class="content has-text-justified">
                                    <p>
                                        In our user study, we show that kinesthetic teaching is preferred due to its intuitive maneuvering; however, data collected with the space mouse enables the usage of force/torque sensor data during inference since all measured disturbances can be attributed
                                        to interactions with the environment. Especially the availability of force/torque sensor data has been shown to provide a 20% improvement in task performance. Overall, when using force/torque data, our system acheives
                                        a 90% success rate.
                                    </p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
    </section>

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@article{stepputtis2022bimanual,
  author    = {Stepputtis, Simon and Bandari, Maryam and Schaal, Stefan and Ben Amor, Heni},
  title     = {A System for Imitation Learning of Contact-Rich Bimanual Manipulation Policies},
  journal   = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year      = {2022},
}</code></pre>
        </div>
    </section>


    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column">
                    <div class="content has-text-centered">
                        <p>
                            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> made by <a href="https://keunhong.com/">Keunhong Park</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>
</body>

</html>